{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "from fairseq import *\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"\" # multiformer or transformer (String)\n",
    "\n",
    "if model_name == \"multiformer\":\n",
    "    from fairseq.models.speech_to_text.s2t_multiformer import S2TMultiformerModel as model_\n",
    "elif model_name == \"transformer\":\n",
    "    from fairseq.models.speech_to_text.s2t_transformer import S2TTransformerModel as model_\n",
    "else:\n",
    "    logger.error(\n",
    "    \"Please choose between the two options: multiformer or transformer\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = \"avg_7_around_best.pt\" # Averaged checkpoint name (String)\n",
    "path = \"\" # Path to where the model's checkpoints are stored (String)\n",
    "data_name_or_path=\"\" # Path to where Must-C lenguage pair is located (String)\n",
    "split = \"\" # Data partition with which to perform the analysis (train_st in the paper) (String)\n",
    "\n",
    "model = model_.from_pretrained(\n",
    "    path,\n",
    "    checkpoint_file=ckpt,\n",
    "    data_name_or_path=data_name_or_path,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "if split not in model.task.datasets.keys():\n",
    "            model.task.load_dataset(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to compute the contribution of each head to he attention output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(split, index):\n",
    "\n",
    "    src_tensor = model.task.dataset(split)[index].source\n",
    "    tgt_tensor = model.task.dataset(split)[index].target\n",
    "\n",
    "    return src_tensor, tgt_tensor\n",
    "\n",
    "\n",
    "def trace_forward(src_tensor, tgt_tensor):\n",
    "\n",
    "    layer_inputs = defaultdict(list)\n",
    "    layer_outputs = defaultdict(list)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    def save_activation(name, mod, inp, out):\n",
    "        layer_inputs[name].append(inp)\n",
    "        layer_outputs[name].append(out)\n",
    "\n",
    "    handles = {}\n",
    "\n",
    "    for name, layer in model.named_modules():\n",
    "        handles[name] = layer.register_forward_hook(partial(save_activation, name))\n",
    "    \n",
    "    src_tensor = src_tensor.unsqueeze(0).to(model.device)\n",
    "    tgt_tensor = torch.cat([\n",
    "        torch.tensor([model.task.tgt_dict.eos_index]),\n",
    "        tgt_tensor[:-1]\n",
    "    ]).unsqueeze(0).to(model.device)\n",
    "\n",
    "    # Inference\n",
    "    model_output, encoder_out = model.models[0](src_tensor, torch.Tensor([src_tensor.size(-2)]), tgt_tensor)\n",
    "    log_probs = model.models[0].get_normalized_probs(model_output, log_probs=True, sample=None)\n",
    "\n",
    "    for k, v in handles.items():\n",
    "        handles[k].remove()\n",
    "\n",
    "    return layer_inputs, layer_outputs\n",
    "\n",
    "\n",
    "def get_layer_contributions(layer, contrib_type, model_name):\n",
    "\n",
    "    if model_name == \"multiformer\":\n",
    "        wo = model.models[0].encoder.transformer_layers[layer].self_attn.wo.weight # (h·d_h) x D\n",
    "        bo = model.models[0].encoder.transformer_layers[layer].self_attn.wo.bias # D\n",
    "\n",
    "        pre_wo = rearrange(\n",
    "            layer_inputs[f\"models.0.encoder.transformer_layers.{layer}.self_attn.wo\"][0][0][0],\n",
    "            't (h c) -> t h c',  \n",
    "            h=model.models[0].encoder.transformer_layers[layer].self_attn.num_heads,\n",
    "        )\n",
    "        pos_wo = layer_outputs[f\"models.0.encoder.transformer_layers.{layer}.self_attn.wo\"][0][0] # T x D\n",
    "\n",
    "    elif model_name == \"transformer\":\n",
    "        wo = model.models[0].encoder.transformer_layers[layer].self_attn.out_proj.weight # (h·d_h) x D\n",
    "        bo = model.models[0].encoder.transformer_layers[layer].self_attn.out_proj.bias # D\n",
    "\n",
    "        pre_wo = rearrange(\n",
    "            layer_inputs[f\"models.0.encoder.transformer_layers.{layer}.self_attn.out_proj\"][0][0],\n",
    "            't b (h c) -> b t h c',  \n",
    "            h=model.models[0].encoder.transformer_layers[layer].self_attn.num_heads,\n",
    "        )[0]\n",
    "\n",
    "        pos_wo = rearrange(\n",
    "            layer_outputs[f\"models.0.encoder.transformer_layers.{layer}.self_attn.out_proj\"][0],\n",
    "            't b c -> b t c',  \n",
    "            c=model.models[0].encoder.transformer_layers[layer].self_attn.embed_dim,\n",
    "        )[0]\n",
    "\n",
    "    else:\n",
    "        logger.error(\n",
    "        \"Please choose between the two options: multiformer or transformer\"\n",
    "        )\n",
    "\n",
    "    wo = rearrange(\n",
    "        wo,\n",
    "        'd (h c) -> h c d',  \n",
    "        h=model.models[0].encoder.transformer_layers[layer].self_attn.num_heads,\n",
    "    )\n",
    "\n",
    "    pos_wo_ = torch.einsum(\n",
    "        't h c , h c d -> t h d',\n",
    "        pre_wo,\n",
    "        wo\n",
    "    )\n",
    "\n",
    "    assert (pos_wo_.sum(-2) + bo - pos_wo).sum() / pos_wo.numel() < 1e-6\n",
    "\n",
    "    if contrib_type == 'norm_l1':\n",
    "        contrib = torch.norm(pos_wo_, dim=-1, p=1)\n",
    "    elif contrib_type == 'norm_l2':\n",
    "        contrib = torch.norm(pos_wo_, dim=-1, p=2)\n",
    "    \n",
    "    contrib = contrib/contrib.sum(-1, keepdim=True)\n",
    "    contrib = contrib.median(-2, keepdim=True).values\n",
    "\n",
    "    return contrib\n",
    "\n",
    "\n",
    "def get_contributions(contrib_type, model_name):\n",
    "    contributions = []\n",
    "    for l in range(len(model.models[0].encoder.transformer_layers)):\n",
    "        contributions.append(get_layer_contributions(l, contrib_type, model_name))\n",
    "    return torch.stack(contributions, dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis with N Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 500 # Integer number of samples to perform the analysis (500 in the paper) (Integer)\n",
    "\n",
    "contributions = []\n",
    "for index in torch.randint(len(model.task.dataset(split)), (num_samples,)).tolist():\n",
    "    src_tensor, tgt_tensor = get_sample(split, index)\n",
    "    layer_inputs, layer_outputs = trace_forward(src_tensor, tgt_tensor)\n",
    "    contributions.append(get_contributions('norm_l1', model_name))\n",
    "\n",
    "contributions = torch.cat(contributions, dim=0)\n",
    "\n",
    "contributions = rearrange(\n",
    "    contributions,\n",
    "    't l h -> l h t',  \n",
    "    h=model.models[0].encoder.transformer_layers[0].self_attn.num_heads,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib_median = contributions.median(-1).values.detach().numpy()\n",
    "ax = sns.heatmap(contrib_median, linewidths=.5, vmin=0.1, vmax=0.5, cmap=\"Blues\",)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
